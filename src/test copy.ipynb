{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzj/miniconda3/envs/lzj/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-31 20:34:07,049] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/data/lzj/LLaMA-Efficient-Tuning/llama-2-70b-chat-hf\"\n",
    "lora_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 15/15 [02:05<00:00,  8.34s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True, quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        ),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_path)\n",
    "generation_config.max_new_tokens = 128\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_k = 50\n",
    "generation_config.top_p = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, \"/data/lzj/LLaMA-Efficient-Tuning/modeltest0731_sft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"<s>Human: \n",
    "We now have the following financial data:\n",
    "\n",
    "moneyflow: Individual stock fund flow\n",
    "trade_daily_data: Daily line chart data\n",
    "income: Financial Income statement data\n",
    "balancesheet: balance sheet\n",
    "\n",
    "What data do you think selecting from above can help us intuitively analyze this user question? When selecting data, please consider the requirements in the user question: 我想要查看平安银行的债务情况。\n",
    "\n",
    "Your answer should meet the following requirements:\n",
    "1. Strictly output in the format [\"Your Choice A\", ...]\n",
    "2. You only need to reply according to the output format without giving any additional information.\n",
    "</s><s>Assistant: \"\"\"\n",
    "inputs = tokenizer(message, return_tensors=\"pt\",add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids'] = inputs['input_ids'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate(**inputs, generation_config=generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Human: \\nWe now have the following financial data:\\n\\nmoneyflow: Individual stock fund flow\\ntrade_daily_data: Daily line chart data\\nincome: Financial Income statement data\\nbalancesheet: balance sheet\\n\\nWhat data do you think selecting from above can help us intuitively analyze this user question? When selecting data, please consider the requirements in the user question: 我想要查看平安银行的债务情况。\\n\\nYour answer should meet the following requirements:\\n1. Strictly output in the format [\"Your Choice A\", ...]\\n2. You only need to reply according to the output format without giving any additional information.\\n</s><s> Assistant: \\n[\"trade_daily_data\", \"balancesheet\"]</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(response[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lzj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
